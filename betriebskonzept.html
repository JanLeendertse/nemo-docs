

<!DOCTYPE html>
<html class="writer-html5" lang="de" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Betriebskonzept bwForCluster NEMO &mdash; NEMO 0.3.2 Dokumentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/translations.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Stichwortverzeichnis" href="genindex.html" />
    <link rel="search" title="Suche" href="search.html" />
    <link rel="next" title="MIT License" href="LICENSE.html" />
    <link rel="prev" title="Betriebskonzept bwForCluster NEMO" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> NEMO
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Dokumentation durchsuchen" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Inhalt:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Betriebskonzept bwForCluster NEMO</a></li>
<li class="toctree-l1"><a class="reference internal" href="#einleitung">Einleitung</a></li>
<li class="toctree-l1"><a class="reference internal" href="#dienstbeschreibung-bwforcluster-nemo">Dienstbeschreibung bwForCluster NEMO</a></li>
<li class="toctree-l1"><a class="reference internal" href="#generelles-sicherheitskonzept">Generelles Sicherheitskonzept</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#security-domains">Security-Domains</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#sicherheitszone-0-physische-absicherung">Sicherheitszone 0: Physische Absicherung</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sicherheitszone-1-zugangsverwaltung-fur-adminstration">Sicherheitszone 1: Zugangsverwaltung für Adminstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sicherheitszone-2-persistenter-und-abgesicherter-speicher">Sicherheitszone 2: Persistenter und abgesicherter Speicher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sicherheitszone-3-dienste">Sicherheitszone 3: Dienste</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sicherheitszone-4-hpc-user">Sicherheitszone 4: HPC-User</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#betriebsmodell">Betriebsmodell</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#administrative-infrastruktur">Administrative Infrastruktur</a></li>
<li class="toctree-l2"><a class="reference internal" href="#knoten-server-und-dienste">Knoten, Server und Dienste</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dienste">Dienste</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ssh">SSH</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scheduler">Scheduler</a></li>
<li class="toctree-l4"><a class="reference internal" href="#http-s">HTTP(S)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dnbd3">DNBD3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ansible">Ansible</a></li>
<li class="toctree-l4"><a class="reference internal" href="#openstack">OpenStack</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dhcp">DHCP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monitoring">Monitoring</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deployment">Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#changemanagement">Changemanagement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wissenschafts-und-home-speicher">Wissenschafts- und HOME-Speicher</a></li>
<li class="toctree-l2"><a class="reference internal" href="#netze">Netze</a></li>
<li class="toctree-l2"><a class="reference internal" href="#zugang-zur-ressource">Zugang zur Ressource</a></li>
<li class="toctree-l2"><a class="reference internal" href="#kontingentierung">Kontingentierung</a></li>
<li class="toctree-l2"><a class="reference internal" href="#administration">Administration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#monitoring-1">Monitoring</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#verantwortlichkeiten">Verantwortlichkeiten</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#maschinensaal-ii-msii">Maschinensaal II (MSII)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#erwartete-ergebnisse">Erwartete Ergebnisse</a></li>
<li class="toctree-l1"><a class="reference internal" href="#ziele-im-sinne-der-informationssicherheit">Ziele im Sinne der Informationssicherheit</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#vertraulichkeit">Vertraulichkeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="#integritat">Integrität</a></li>
<li class="toctree-l2"><a class="reference internal" href="#verfugbarkeit-belastbarkeit">Verfügbarkeit, Belastbarkeit</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regelmaszige-uberprufung-bewertung-evaluation">Regelmäßige Überprüfung, Bewertung, Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#bezuge-zu-konzepten-richtlinien-und-verordnungen">Bezüge zu Konzepten, Richtlinien und Verordnungen</a></li>
<li class="toctree-l1"><a class="reference internal" href="LICENSE.html">MIT License</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NEMO</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Betriebskonzept bwForCluster NEMO</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/betriebskonzept.rst.txt" rel="nofollow"> Quelltext anzeigen</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="betriebskonzept-bwforcluster-nemo">
<h1>Betriebskonzept bwForCluster NEMO<a class="headerlink" href="#betriebskonzept-bwforcluster-nemo" title="Link zu dieser Überschrift">¶</a></h1>
<p>eScience, Rechenzentrum, Albert-Ludwigs-Universität Freiburg</p>
<p>Michael Janczyk (MJ), Jan Leendertse (JL), Dirk von Suchodoletz (DvS), Bernd Wiebelt (BW)</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 57%" />
<col style="width: 43%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Vorlage an:</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Vorgelegt am:</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Gültig ab:</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Version:</p></td>
<td><p>0.3.2</p></td>
</tr>
<tr class="row-odd"><td><p>Datum:</p></td>
<td><p>25.03.2021</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warnung</p>
<dl class="simple">
<dt>TLP:AMBER</dt><dd><p>Limited disclosure, restricted to participants‘ organizations. Distribution outside this audience requires written permission from the originator.</p>
</dd>
</dl>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 77%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Version</p></th>
<th class="head"><p>Datum</p></th>
<th class="head"><p>Autor*innen</p></th>
<th class="head"><p>Änderungen</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0.3.2</p></td>
<td><p>25.03.2021</p></td>
<td><p>MJ</p></td>
<td><p>Korrekturen, Ergänzungen.</p></td>
</tr>
<tr class="row-odd"><td><p>0.3.1</p></td>
<td><p>19.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Korrekturen, Ergänzungen.</p></td>
</tr>
<tr class="row-even"><td><p>0.3.0</p></td>
<td><p>13.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Umstrukturierung des Textes, stärkere Verknüpfung von Sicherheit und Betriebsmodell, Ergänzungen.</p></td>
</tr>
<tr class="row-odd"><td><p>0.2.1</p></td>
<td><p>09.03.2021</p></td>
<td><p>BW, JL, DvS, MJ</p></td>
<td><p>Anmerkungen und Korrekturen.</p></td>
</tr>
<tr class="row-even"><td><p>0.2.0</p></td>
<td><p>09.03.2021</p></td>
<td><p>DvS</p></td>
<td><p>Ergänzungen u.a. aus dem DFN-Paper zu Security-Domains und Absicherungskonzept.</p></td>
</tr>
<tr class="row-odd"><td><p>0.1.0</p></td>
<td><p>06.03.2021</p></td>
<td><p>MJ</p></td>
<td><p>Erster Entwurf basierend auf Informationen des Betriebskonzepts de.NBI-FR und einer Vorlage von TOMs aus Tübingen.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="einleitung">
<h1>Einleitung<a class="headerlink" href="#einleitung" title="Link zu dieser Überschrift">¶</a></h1>
<p>Das Konzept, auf dessen Grundlage das bwForCluster NEMO betrieben wird,
geht auf Überlegungen in der Abteilung eScience des Rechenzentrums der
Universität Freiburg (RZ) zurück und wurde in verschiedenen Projekten
iterativ entwickelt. <a class="footnote-reference brackets" href="#id26" id="id1">1</a> Die für den bwForCluster NEMO beschaffte
Hardware – Rechenknoten, Speichersystem für wissenschaftliche Daten und
Infrastrukturserver – wird in Datenschränken untergebracht, die vom RZ
in hierfür dedizierten Räumlichkeiten betrieben werden. Die
Datenschränke werden vom RZ betreut und auf grundlegende
Betriebsbereitschaft (Strom, Kälte) überwacht. Es werden primär zwei
große Speichersysteme genutzt, einerseits eine professionelle
Storage-Appliance für administrative und Nutzer*innendaten, andererseits
ein schnelles paralleles Dateisystem für die Nutzung durch Compute-Jobs.
Auf den Infrastrukturservern laufen diverse Dienste, die skriptgesteuert
mit Rezepten installiert und konfiguriert werden. Die Rechenknoten
werden via netzwerkbasiertem Remote-Boot von dedizierten
Infrastrukturservern zustandslos betrieben. Auf diesen Knoten werden die
Jobs der Wissenschaftler*innen ausgeführt, teilweise auch in VFUs und
CFUs. In diesen VFUs und CFUs laufen die Anwendungen oder Umgebungen,
die von den Wissenschafter*innen kontrolliert werden.</p>
<p>Es wird auf allgemeine Prinzipien des High Performance Computings (HPC)
zurückgegriffen, die um weitere Elemente erweitert werden. Besonders der
Anspruch, Daten über längere Zeiträume hinweg in ihren Entstehungs- und
Verarbeitungskontexten reproduzierbar zu halten, führte zu einer
Komplementierung des traditionellen Betriebsmodells von HPC-Systemen um
Virtualisierung. Das bwForCluster NEMO offeriert über den traditionellen
Baremetal-Betrieb hinaus Dienste für „Virtualisierte
Forschungsumgebungen“ (VFU), die auf HPC-Ressourcen lauffähig sind gemäß
einem Betriebsmodell, das von Mitarbeitern des RZ seit mehreren Jahren
in verschiedenen Projekten erprobt und in den produktiven Betrieb
übernommen wurde
dfn-forum-2017,bwhpc2018:vicevre,ViCE2019,ViCE2019a.
Innerhalb von VFUs haben Wissenschaftler*innen die Freiheit,
Forschungsdaten und deren Kontext so zu abstrahieren, dass sie auf
anderen Systemen weiter ausgeführt und reproduziert werden können.
Dieses Methodik wurde in den letzten Jahren zu den „Containerisierten
Forschungsumgebungen“ (CFU) weiterentwickelt. Diese Umgebungen sind
weniger komplex und mit einfachen Rezepten – beispielsweise den
Singularity Definition Files, die eine Container-Umgebung beschreiben
und verwendet werden um diese zu generieren – herzustellen.</p>
<p>Mit den VFU und CFU werden Hardware, Software und Dienste so entkoppelt,
dass für den bwForCluster NEMO ein verbessertes Management von
Ressourcen erreicht wird. Ein Grundgedanke dieser konzeptionellen
Vorüberlegungen ist die Aufteilung in Schichten. Der hier vorgestellte
Dienst des bwForClusters NEMO deckt im Modell, das in
Meier:2017,konrad-werk vorgestellt wurde, die
Ebene der IT-Ressourcen ab. Für die Betriebsorganisation wird sie
innerhalb des bwForClusters NEMO weiter differenziert in Schrank,
Hardware und Dienste. Dieses Konzeptpapier beschreibt den Betrieb des
bwForClusters NEMO.</p>
</div>
<div class="section" id="dienstbeschreibung-bwforcluster-nemo">
<h1>Dienstbeschreibung bwForCluster NEMO<a class="headerlink" href="#dienstbeschreibung-bwforcluster-nemo" title="Link zu dieser Überschrift">¶</a></h1>
<p>Es liegt eine Dienstbeschreibung für das HPC-Computing-Angebot des
Rechenzentrums im Rahmen des allgemeinen Servicekatalogs vor. Diese kann
online von den Seiten des Rechenzentrums abgerufen werden. <a class="footnote-reference brackets" href="#id27" id="id2">2</a> Diese
Dienstbeschreibung wird einem regelmäßigen Review-Prozess in der Runde
der Abteilungsleiter*innen unterzogen.</p>
</div>
<div class="section" id="generelles-sicherheitskonzept">
<h1>Generelles Sicherheitskonzept<a class="headerlink" href="#generelles-sicherheitskonzept" title="Link zu dieser Überschrift">¶</a></h1>
<p>Das Sicherheitskonzept der Abteilung eScience strebt an, die Fähigkeiten
zur Business-Continuity von großen, verteilten IT-Infrastrukturen durch
technische Vorbereitungen schrittweise zu verbessern und damit das
Schutzziel der Verfügbarkeit durch einen geplanten Prozess zu verfolgen
dfn-sec-2021. Die Schutzziele Integrität und
Vertraulichkeit werden in die Überlegungen einbezogen. Das umfasst eine
Reihe von Grundgedanken und Konzepte, wie Angriffen vorgebeugt und eine
flächendeckende und skalierende Wiederherstellung arbeitsfähiger
Compute-Infrastrukturen vorbereitet werden kann. Die angestrebte Lösung
beruht im wesentlichen auf drei Säulen: einer klaren funktionalen und
Netzwerkseparierung, einem reproduzierbaren Maschinen-Deployment mit
abgesichertem Speichersystem und einer Sicherstellung der
Datenintegrität auf einem geschützten Persistenzlayer. Ein zentraler
Baustein des zuverlässigen und abgesicherten Deployments basiert auf
einem netzbasiertem Bootsystem mit „Stateless System Remote-Boot“
hpc-remote-boot,bwhpc2018:dnbd3.</p>
<div class="section" id="security-domains">
<h2>Security-Domains<a class="headerlink" href="#security-domains" title="Link zu dieser Überschrift">¶</a></h2>
<p>Als System, auf das aus dem Internet zugegriffen wird, ist das
HPC-Cluster laufenden Attacken ausgesetzt, die automatisiert nach
Schwachstellen scannen. Es kann nie mit Sicherheit ausgeschlossen
werden, dass einer der zahlreichen Angriffe erfolgreich ist, entweder da
er gezielt stattfindet, oder weil aus rein statistischen Gründen eine
noch nicht einschlägig bekannte Lücke automatisiert gefunden wird. In
einem solchen Fall ist es bedeutsam, dass der Angriff nicht sofort in
einem Domino-Effekt auf den gesamten Cluster durchschlägt.</p>
<p>Es existieren verschieden Methoden, das Risiko durch Angriffe auf
Systeme und Infrastrukturen zu verringern. Für den HPC-Cluster geschieht
die Reduktion des Risikos auf mehreren Ebenen: klare Rechte- und
Netzwerkseparierung, reproduzierbares Maschinen-Deployment mit
abgesicherten Speichersystem, Sicherstellung der Datenintegrität auf
einem geschützten Persistenzlayer.</p>
<p>In dem Augenblick, in dem die überwiegende Anzahl von Maschinen im
Cluster als Angriffsvektor aus dem Spiel genommen werden, lassen sich
kritische Systeme leichter identifizieren und schützen. Maßnahmen im
Arbeitsfeld der Business-Continuity fokussieren sich auf auf eine
überschaubare Anzahl von Systemvarianten und Ressourcen wie
beispielsweise die Boot-Kette aus TFTP, HTTP(S) und DNBD3-Services. <a class="footnote-reference brackets" href="#id28" id="id3">3</a></p>
<p>Es wird in Hinblick auf die Planungen für den Nachfolge-Cluster eine
gestufte, hierarchische Sicherheitsarchitektur angestrebt. Dabei werden
verschiedene Sicherheitsbereiche oder Sicherheitszonen definiert, wobei
die Übergänge zwischen den Zonen klar und scharf definiert sind. Die
Hindernisse, die ein Angreifer überwinden muss, um sich zwischen den
Zonen zu bewegen, steigen mit zunehmendem Schutzbedarf. Für das
HPC-System wurden die folgenden Sicherheitszonen identifiziert:</p>
<ul class="simple">
<li><p>Sicherheitszone 0: Physische Absicherung</p></li>
<li><p>Sicherheitszone 1: Zugangsverwaltung für Adminstration</p></li>
<li><p>Sicherheitszone 2: Persistenter und abgesicherter Speicher</p></li>
<li><p>Sicherheitszone 3: Dienste</p></li>
<li><p>Sicherheitszone 4: HPC-User</p></li>
</ul>
<p>Ein Hauptaspekt bei der Ausgestaltung der Sicherheitszonen ist die
Separierung der Netzwerke, sodass Kommunikation zwischen den Netzwerken
auf das Notwendige beschränkt wird und es möglichst wenige
Kommunikationspfade gibt, die Zonen überspringen. Wo es solche Übergänge
gibt, müssen sie aus wohldefinierten mit klar vorgegebenen Daten- und
Informationsflussrichtungen bestehen. Technisch lässt sich das durch den
Einsatz von privaten Netzwerken mit eingeschränktem Routing,
VLAN-Definitionen für die Netzwerkports auf den Switchen und dem Einsatz
von Firewalls und Proxys erreichen.</p>
<div class="section" id="sicherheitszone-0-physische-absicherung">
<h3>Sicherheitszone 0: Physische Absicherung<a class="headerlink" href="#sicherheitszone-0-physische-absicherung" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Rechenknoten, Server und die zugehörigen Speichersysteme sind in
wassergekühlten Schränken im Maschinensaal II (MSII) untergebracht. <a class="footnote-reference brackets" href="#id29" id="id4">4</a>
Speicher und Service-Knoten sind zusätzlich redundant gegen
Stromausfälle gesichert und können bis zu einem sicheren Herunterfahren
betrieben werden. Die Bedingungen führt die Dienstbeschreibung zum
„Machine-Hosting“ näher aus MachHost:2020.</p>
<p>Das Speichersystem für Home-Verzeichnisse und administrative Daten
Dell-EMC Isilon ist gegen Ausfälle neben dem Hauptstandort
MSII (Betriebsstandort A) noch in einem weiteren externen Serverraum
(Betriebsstandort B, KG II) redundant gesichert.</p>
</div>
<div class="section" id="sicherheitszone-1-zugangsverwaltung-fur-adminstration">
<h3>Sicherheitszone 1: Zugangsverwaltung für Adminstration<a class="headerlink" href="#sicherheitszone-1-zugangsverwaltung-fur-adminstration" title="Link zu dieser Überschrift">¶</a></h3>
<p>Letztlich wird eine Infrastruktur durch Menschen verwaltet, die sich in
irgendeiner Form während ihrer Tätigkeit authentifizieren und
autorisieren müssen. Bei großen Infrastrukturen wird zudem ein Rollen-
und Rechtemanagement notwendig sein, weil nicht jeder Administrator die
gleichen Rechte benötigt oder bekommen soll. Insofern ist diese
Sicherheitszone mit maximalen Einschränkungen zu versehen und
bestmöglich abzusichern.</p>
<p>Eine wesentliche Aufgabe dieser Zone liegt in der Verwaltung von
Passwörtern und SSH-Schlüsseln. Passwörter und Schlüssel können dann auf
untergeordnete Sicherheitszonen per Push-Verfahren verteilt werden.
Alternativ könnte hier eine Zertifizierungsstelle angesiedelt sein, die
Zugangs-Schlüssel zertifizieren kann oder beschränkt gültige
Zugangstoken (beispielsweise signierte SSH-Schlüssel) generiert und
ausgibt.</p>
<p>Die Server in dieser Zone sollten eine physische Präsenz besitzen oder
mit einfacher Virtualisierung oder Containerisierung (libvirt, docker)
realisiert werden. Der Einsatz einer komplexen
Virtualisierungsinfrastruktur (beispielsweise OpenStack) würde
weitergehende rekursive Maßnahmen erfordern.</p>
<p>Der Zugang zu dieser Sicherheitszone muss sowohl physisch als auch
logisch stark eingeschränkt sein. Das heißt, dass die Server mit einer
mechanischen Zugangssperre versehen werden müssen, sodass nur
berechtigte Personen physisch Zugriff erhalten können. Für den logischen
Zugriff via Netzwerk müssen die zulässigen Netzwerkbereiche und
Protokolle auf das notwendige Minimum reduziert werden. „Push-Requests“,
von dieser Sicherheitszone initiierte Verbindungen an untergeordnerte
Zonen, sind unproblematischer als „Pull-Requests“, also von
untergeordneten Zonen initiierte Verbindungen. Für letztere sollten nach
Möglichkeit Proxy-Lösungen zum Einsatz kommen. Idealerweise beschränkt
sich die Kommunikation auf jeweils benachbarte Zonen.</p>
</div>
<div class="section" id="sicherheitszone-2-persistenter-und-abgesicherter-speicher">
<h3>Sicherheitszone 2: Persistenter und abgesicherter Speicher<a class="headerlink" href="#sicherheitszone-2-persistenter-und-abgesicherter-speicher" title="Link zu dieser Überschrift">¶</a></h3>
<p>Im persistenten und abgesicherten Speicher werden unter anderem
Konfigurationen, Build-Rezepte und Datenbanken abgelegt, die für das
Aufsetzen der Dienste in nachgeordneten Zonen benötigt werden.
„Abgesicherter Speicher“ versteht sich in diesem Fall auf zweifache Art:
Zum einen als Speichersystem, das gegen unbeabsichtigten Verlust oder
Verfälschung der Daten Vorkehrungen trifft, zum anderen als Speicher,
der gegen absichtliche Angriffe von Dritten besonders geschützt ist.</p>
<p>Zum Einsatz kommen an dieser Stelle professionelle Storage-Systeme mit
einer hohen geo-redundanten Absicherung mit eventuellem Tape-Backup und
gestaffelter Versionierung von Daten, beispielsweise durch Snapshots.
Dabei werden die integralen Mechanismen des Speichersystems genutzt, um
Verfügbarkeit, Integrität und bei Bedarf Vertraulichkeit
sicherzustellen. Die Systeme laufen in abgetrennten, eigenen
Netzwerkbereichen mit wenigen wohldefinierten und kontrollierten
Übergängen zu den nachgeordneten Sicherheitszonen, wobei idealerweise
nur die direkt darunterliegende Zone direkt versorgt wird.</p>
<p>Prinzipiell sind mehrere Arten von Aufgaben und damit von Datenzugriff
und Datenflussrichtung zu unterscheiden:</p>
<ul class="simple">
<li><p>Lesender und schreibender Zugriff auf Datenbanken und
Konfigurationen, die in der Sicherheitszone 3 (Dienste) benötigt
werden. Diese Methode wird für Laufzeitdaten der Dienste genutzt.</p></li>
<li><p>Ein Nur-Lese-Zugriff (Readonly) kann für System-Images und andere
nicht durch nachgeordnete Sicherheitszonen zu verändernde Daten
angeboten werden.</p></li>
<li><p>Ein Nur-Hinzufügen-Zugriff (Append-Only) ist beispielsweise für Log-
und Monitoringdaten relevant, um nachträgliche unerkennbare
Modifikationen zu unterbinden.</p></li>
</ul>
<p>Über die systemisch verfügbaren Sicherheitsmerkmale hinaus können zudem
zonenübergreifende Methoden eingesetzt werden. Im Falle von
System-Images können beispielsweise alle erzeugten System-Images sowie
relevanten Konfigurationen auf einem gesicherten Fileserver abgelegt
werden, der mit Versionierung beispielsweise durch Snapshots arbeitet.
Die Unveränderlichkeit der System-Images könnte in Zukunft zusätzlich
durch kryptografische Prüfsummen abgesichert werden. Die Server wäre für
nachgeordnete Zonen ausschließlich für die den Client-Betrieb
notwendigen Ports freigegeben. Sie können nur aus speziellen Netzen für
die Konfiguration erreicht werden.</p>
</div>
<div class="section" id="sicherheitszone-3-dienste">
<h3>Sicherheitszone 3: Dienste<a class="headerlink" href="#sicherheitszone-3-dienste" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die Sicherheitszone Dienste definiert die Erreichbarkeit der
Infrastruktur und die Art, wie Dienste ausgerollt und betrieben werden.
Bei der Compute-Infrastruktur des HPC-Clusters wird zwischen Servern,
die öffentliche Dienste anbieten, und internen Servern unterschieden.
Die internen Server bestehen aus Infrastruktur-relevanten Komponenten,
wie Storage-, Boot- und Build-Servern, sowie Servern, die API-Dienste,
die keine öffentliche Erreichbarkeit benötigen, vorhalten. Sie befinden
sich in nach außen hin abgekapselten privaten Netzen mit nur wenigen
manuell freigeschalteten Zutrittspunkten.</p>
<p>Auf allen Servern sind stark restriktive Firewall-Regeln aktiv, die
zusätzlich zur Separierung auf Netzwerkebene die Zugriffsmöglichkeiten
(und damit auch die potentiellen Angriffsvektoren) reduzieren.
Prinzipiell sind lediglich die notwendigen Ports der erforderlichen
Protokolle in den notwendigen Netzen freigeschaltet. SSH nimmt als
erster Angriffspunkt eine Sonderrolle ein. Allen Servern ist gemein,
dass sie lediglich SSH-Verbindungen von innerhalb des Projektes (und
damit aus klar definierten Netzen) kommend annehmen. Lediglich die
Einstiegspunkte für die Administration, beispielsweise über
Proxy-Jump-Knoten, in die Compute-Infrastrukturen stellen eine Ausnahme
dar.</p>
<p>Diese Netzwerkseparierung und die restriktiven Firewall-Regeln
gewährleisten, dass die Anzahl der Angriffsvektoren so gering wie
möglich ist und Übergriffe von anderen möglicherweise befallenen
Compute-Infrastrukturen unterbindet. Um die Auswirkungen von Angriffen
auch bei einer großen Serveranzahl gering zu halten, wird viel Wert auf
eine schnelle Wiederherstellbarkeit der Dienste gelegt.</p>
</div>
<div class="section" id="sicherheitszone-4-hpc-user">
<h3>Sicherheitszone 4: HPC-User<a class="headerlink" href="#sicherheitszone-4-hpc-user" title="Link zu dieser Überschrift">¶</a></h3>
<p>Die äußerste Zone zeichnet sich dadurch aus, dass Nutzer*innen aus den
Gruppen Forschende oder Mitarbeitende direkt oder indirekt Zugriff
haben. Sie nutzen primär SSH, um mit dem HPC-Cluster zu interagieren.
Der Zugriff beschränkt sich auf die zwei im öffentlichen Netz
exponierten Login-Knoten zum Abschicken von Jobs und zwei
Visualisierungs-Knoten. Zur Absicherung des Zugangs durch HPC-Nutzer
wird auf eine Zwei-Faktor-Authentifizierung gesetzt.</p>
</div>
</div>
</div>
<div class="section" id="betriebsmodell">
<h1>Betriebsmodell<a class="headerlink" href="#betriebsmodell" title="Link zu dieser Überschrift">¶</a></h1>
<p>Das Betriebsmodell beschreibt konkrete Schritte des Deployments und der
täglichen Produktion des HPC-Clusters. Hierzu wird eine Kombination aus
administrativen und für Compute genutzte Server eingesetzt.</p>
<p>Aussagen zu Installation und Softwareupdates</p>
<div class="section" id="administrative-infrastruktur">
<h2>Administrative Infrastruktur<a class="headerlink" href="#administrative-infrastruktur" title="Link zu dieser Überschrift">¶</a></h2>
<p>Dem Security-Konzept folgend was zur Persistenzschicht sagen.</p>
<p>Eventuell Server und Dienste vom nächsten Abschnitt hier reinziehen!?</p>
</div>
<div class="section" id="knoten-server-und-dienste">
<h2>Knoten, Server und Dienste<a class="headerlink" href="#knoten-server-und-dienste" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die installierte Hardware des bwForCluster NEMO besteht aus über 900
Rechenknoten und einigen dedizierten Servern für NEMO-Dienste.
Zusätzlich können virtuelle Maschinen als „Virtualisierte
Forschungsumgebungen“ (VFU) in einer Openstack-Cloud-Umgebung gestartet
werden. Diese werden auf den gleichen über 900 Rechenknoten ausgeführt,
wie reguläre Cluster-Jobs. Zugang zum Cluster erfolgt über sogenannte
Login-Knoten,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>login1.nemo.uni-freiburg.de <span class="o">(</span><span class="nb">alias</span> login.nemo.uni-freiburg.de<span class="o">)</span>
login2.nemo.uni-freiburg.de
</pre></div>
</div>
<p>den Visualisierungsknoten (Vis),</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>vis1.nemo.uni-freiburg.de
vis2.nemo.uni-freiburg.de
</pre></div>
</div>
<p>und über das Openstack-Dashboard. Die Zugangsknoten sind im öffentlichen
Internet exponiert, welches jedoch auf das Belwü-Netz eingeschränkt
wurde. <a class="footnote-reference brackets" href="#id30" id="id5">5</a> Der Zugriff erfolgt primär mittels SSH-Dienst, das
Openstack-Dashboard über verschlüsseltes HTTPS.</p>
<div class="section" id="dienste">
<h3>Dienste<a class="headerlink" href="#dienste" title="Link zu dieser Überschrift">¶</a></h3>
<div class="section" id="ssh">
<h4>SSH<a class="headerlink" href="#ssh" title="Link zu dieser Überschrift">¶</a></h4>
<p>Dieser Dienst läuft auf allen Knoten und Servern und erlaubt den Login
von Wisschenschaftler*innen und Administrator*innen auf diesen.</p>
</div>
<div class="section" id="scheduler">
<h4>Scheduler<a class="headerlink" href="#scheduler" title="Link zu dieser Überschrift">¶</a></h4>
<p>Dieser Dienst ist auf dem Management-Server von NEMO aktiv und dient zum
„Scheduling“ (Verteilen nach vorgegebenem Algorithmus) von Jobs auf dem
Cluster. Dazu sind auf den Rechenknoten Clients installiert, die Jobs
und Ressourcenverbrauch protokollieren und diese Information an den
Scheduler zurückmelden.</p>
</div>
<div class="section" id="http-s">
<h4>HTTP(S)<a class="headerlink" href="#http-s" title="Link zu dieser Überschrift">¶</a></h4>
<p>Das OpenStack-Dashboard ist als Webschnittstelle umgesetzt und setzt für
den Zugriff auf HTTPS, um eine Absicherung bei der Nutzung über das
öffentliche Belwü-Netz zu erreichen. Auf dem Deployment-Server wird HTTP
verwendet um Konfigurationen zu den Rechenknoten zu verteilen (Teil des
iPXE-basierten Boot-Ablaufs und der individuellen Knotenkonfiguration).
Die Deployment-Server sind nur im internen NEMO-Netz erreichbar.</p>
</div>
<div class="section" id="dnbd3">
<h4>DNBD3<a class="headerlink" href="#dnbd3" title="Link zu dieser Überschrift">¶</a></h4>
<p>Auf den Deployment-Servern laufen zwei DNBD3-Instanzen. Dies stellt
sicher, dass wenn einer dieser Server ausfällt, das Cluster weiterhin
mit dem Betriebssystem-Image versorgt wird.</p>
</div>
<div class="section" id="ansible">
<h4>Ansible<a class="headerlink" href="#ansible" title="Link zu dieser Überschrift">¶</a></h4>
<p>Auf dem Management-Server übernimmt Ansible das Ausrollen der Dienste
und deren Konfiguration.</p>
</div>
<div class="section" id="openstack">
<h4>OpenStack<a class="headerlink" href="#openstack" title="Link zu dieser Überschrift">¶</a></h4>
<p>Mehrere Openstack-Server und -Dienste sind Cluster-intern für die
Nutzung von VFUs zuständig.</p>
</div>
<div class="section" id="dhcp">
<h4>DHCP<a class="headerlink" href="#dhcp" title="Link zu dieser Überschrift">¶</a></h4>
<p>Die IP-Adressen werden bei Rechen-, Login, sowie Visualisierungs-Knoten
über DHCP verteilt. Dieser Dienst wird von der Netzwerkabteilung
mithilfe der Appliance Infoblox betrieben.</p>
</div>
<div class="section" id="monitoring">
<h4>Monitoring<a class="headerlink" href="#monitoring" title="Link zu dieser Überschrift">¶</a></h4>
<p>Der Monitoring-Server empfängt und speichert alle Log- und
Protokoll-Dateien. Hierbei werden Login-Versuche, kritische Fehler und
Hardware-Parameter protokolliert und teilweise visualisiert. Für
einfache Parameter wie Temperatur eines Knotens sind Grenzwerte
definiert. Bei Überschreitung dieser werden per Mail die
Administrator*innen des Clusters verständigt.</p>
</div>
</div>
</div>
<div class="section" id="deployment">
<h2>Deployment<a class="headerlink" href="#deployment" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die Erzeugung und das Deployment der Server geschieht in einem
mehrstufigen Prozess, der konzeptionell in den Sicherheitszonen 3 und 4
ähnlich abläuft. Das Gerüst für die Prozesse in den Zonen 3 und 4 bilden
vorbereitete Skripte und Konfigurationen aus der Zone 2. Sie ermöglichen
eine rasche Wiederherstellung von Servern und Diensten in den Zonen 3
und 4. Von den Maschinen in Zone 3 (Dienste) gibt es wenige Ausformungen
von Instanzen mit individuellen Konfigurationen. In der
Sicherheitszone 4 (HPC-User) liegen deutlich mehr Instanzen, die sich
jedoch auf wenige Vorlagen für HPC-Knoten beziehen. Es existieren
weniger individuelle Konfigurationen, und für den Neustart von Instanzen
ist ein Rückgriff auf gespeicherte Zustände nicht notwendig.</p>
<p>Die Dienste beim bwForCluster NEMO werden über Ansible-Rollen auf den
Serverknoten aufgesetzt. Das ermöglicht ein schnelles und einfaches
ausrollen auf neuen Servern. Es müssen nur wenige Anpassungen
durchgeführt werden.</p>
<p>Die Rechenknoten werden ebenfalls mittels Ansible erzeugt. Hierzu wird
das CentOS7-Vorlagen-Image mit Ansible konfiguriert und in in ein
lesbares QCOW2-Image konvertiert. Mit dem in der Abteilung „eScience“
entwickelten Boot-Framework wird dann das Image über das Netzwerk
gestartet. Das Image wird dabei über das nur lesbare Blockdevice DNBD3
eingebunden. Für Schreiboperationen wird eine copy-on-write Schicht
darüber gelegt, die bei jedem Boot eines Knotens frisch initialisiert
wird. Alle neu generierten Images bekommen eine inkrementierte
Revisionsnummer, so dass die Umgebung zum einen reproduzierbar ist, zum
anderen bei Problemen mit einer Revision einfach auf eine ältere zurück
gegriffen werden kann.</p>
<p>Die Entscheidung, welche Systemversion, Revision und Konfiguation
geladen wird, trifft der sogenannte Bootauswahlserver anhand der
Zugehörigkeit der MAC-Adresse der Netzwerkkarte, über die der initiale
Start lief, zu einer Boot-Gruppe
bwhpc2018:sortinghat. Diese Information wird
jedesmal beim Boot ausgewertet. Die Boot-Gruppe entscheidet über die
Konfiguration des Knotens. Sie wird verwendet, um spezielle Knoten zu
konfigurieren, beispielsweise bei GPU-Knoten. Bei neuer Hardware durch
Neubeschaffungen oder Ersatz bei Reparaturen muss lediglich die
MAC-Adresse einer Gruppe zugeordnet werden. Neue Konfigurationen können
ebenfalls schnell eingerichtet werden, da nur die zur Basisgruppe
unterschiedliche Konfiguration vorgenommen werden muss.</p>
</div>
<div class="section" id="changemanagement">
<h2>Changemanagement<a class="headerlink" href="#changemanagement" title="Link zu dieser Überschrift">¶</a></h2>
<p>Der Deploymentprozess erleichtert das Changemanagement. Die
Bereitstellung des Basissystems erlaubt schnelle Funktionstests, da beim
Netzwerk-Boot lediglich die neuere Version angefahren werden muss. Die
Hardwaregrundlage der Rechenknoten verändert sich im Laufe der
Beschaffungszyklen, jedoch wird im Beschaffungsprozess und beim Design
des Basissystems darauf geachtet, dass neue Knoten ohne Brüche in das
Grundsystem übernommen werden können. Die Heterogenität wird durch den
kontinuierlichen Austausch von Hardware verursacht, für die jeweils die
zum Moment der Beschaffung günstigsten oder passendsten Komponenten
verwendet werden.</p>
<p>Für jede Geräteklasse wird ein Knoten reserviert, mit dem ausschließlich
Tests durchgeführt werden. Erst wenn bei Änderungen am Grundsystem oder
Patches auf den reservierten Knoten durchgetestet wurden, werden diese
Änderungen auf den produktiven Knoten ausgerollt.</p>
</div>
<div class="section" id="wissenschafts-und-home-speicher">
<h2>Wissenschafts- und HOME-Speicher<a class="headerlink" href="#wissenschafts-und-home-speicher" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die HOME-Verzeichnisse der Nutzer*innen liegen auf dem Isilon-Speicher
der Universität. <a class="footnote-reference brackets" href="#id31" id="id6">6</a> Für die aktuell verarbeiteten wissenschaftlichen
Daten dient ein zentraler Parallelspeicher, der auf BeeGFS
aufsetzt. <a class="footnote-reference brackets" href="#id32" id="id7">7</a> Dieser Speicher ist neben dem bwForCluster NEMO ebenfalls
in der ATLAS-Umgebung eingebunden. Diese beinhaltet das
ATLAS-Cluster <a class="footnote-reference brackets" href="#id33" id="id8">8</a> und die ATLAS-VFU. <a class="footnote-reference brackets" href="#id34" id="id9">9</a> Dadurch können zusätzlich
Nutzer*innen und Administratoren der Freiburger ATLAS-Gruppen auf diesen
Speicher zugreifen.</p>
<p>Nutzer*innen können in der Standardeinstellung nur ihre Daten einsehen
und bearbeiten. Administratoren können alle Daten einsehen, sofern sie
nicht Nutzer- oder Client-seitig verschlüsselt wurden, und bearbeiten.
Beide Speicher werden nicht standardmäßig verschlüsselt.</p>
</div>
<div class="section" id="netze">
<h2>Netze<a class="headerlink" href="#netze" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die Netzwerkanbindung der Serverchränke im Maschinensaal und der
zentralen Switche wird von der Abteilung „eScience“ in Zusammenarbeit
mit der Abteilung „Netze und Kommunikationsdienste“ (Netzabteilung) im
RZ durchgeführt. Diese Anbindung erlaubt eine Administration der Knoten
in den Schränken von festgelegten IP-Adressen aus, die nur in Räumen der
Universität Freiburg sowie über VPN-Verbindungen zugewiesen werden.</p>
<p>Die internen Uni-Netzwerke für das bwForCluster NEMO, die VFUs, das
ATLAS-Cluster und die Isilon sind voneinander getrennt und lassen nur
Zugriff von zum Betrieb notwendigen Netzen zu. Welche dies im einzelnen
sind, müssen vom jeweiligen Dienst erfragt werden.</p>
<p>Das bwForCluster NEMO verwendet folgende Netze:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">10</span>.16.0.0/16          NEMO: Rechenknoten, Server und Parallelspeicher
                            Login- und Vis-Knoten über interne Netzwerkschnittstelle
<span class="m">132</span>.230.222.0/24      NEMO: Login- und Visualisierungsknoten
<span class="m">10</span>.17.0.0/16          NEMO: CMS-VFU
<span class="m">10</span>.18.0.0/16          NEMO: ATLAS-VFU
<span class="m">10</span>.20.0.0/21          NEMO: NEMO-VFU <span class="o">(</span>unused<span class="o">)</span>
<span class="m">10</span>.20.8.0/21          NEMO: NEMO-VFU <span class="o">(</span>unused<span class="o">)</span>
<span class="m">10</span>.20.16.0/21         NEMO: NEMO-VFU <span class="o">(</span>unused<span class="o">)</span>
<span class="m">10</span>.20.24.0/21         NEMO: NEMO-VFU <span class="o">(</span>unused<span class="o">)</span>
<span class="m">10</span>.20.32.0/21         NEMO: NEMO-VFU <span class="o">(</span>unused<span class="o">)</span>
<span class="m">10</span>.20.40.0/21         NEMO: ATLAS-TEST-VFU
</pre></div>
</div>
<p>Obige Netze sind jeweils voneinander getrennt. Lediglich die ATLAS-VFU
und ATLAS-TEST-VFU können zusätzlich auf das NEMO-Netz <code class="docutils literal notranslate"><span class="pre">10.16.0.0/16</span></code>
zugreifen. Das Cluster kann ansonsten nur über die öffentliche
IP-Adressen der Login- und Vis-Knoten erreicht werden. Die Rechenknoten
sind mit mindestens versorgt, Server, die Dienste anbieten, sind mit
mindestens zwei Anschlüssen mit über das Link Aggregation Control
Protocol (LACP) an zwei Top-Level-Switche angebunden. <a class="footnote-reference brackets" href="#id35" id="id10">10</a> Zusätzlich
sind alle Rechenknoten mit dem Hochgeschwindigkeitsnetzwerk „Omni-Path“
mit miteinander und dem wissenschaftlichen Parallelspeicher
verbunden. <a class="footnote-reference brackets" href="#id36" id="id11">11</a></p>
</div>
<div class="section" id="zugang-zur-ressource">
<h2>Zugang zur Ressource<a class="headerlink" href="#zugang-zur-ressource" title="Link zu dieser Überschrift">¶</a></h2>
<p>Zugang zum bwForCluster NEMO haben lediglich registrierte Nutzer*innen
des bwForClusters NEMO. Antragsberechtigt sind nur Wissenschaftler*innen
aus Baden-Württemberg. Die genauen Zugangskriterien und die einzelnen
Schritte der Registrierungsprozedur sind im bwHPC-Wiki
beschreiben. <a class="footnote-reference brackets" href="#id37" id="id12">12</a></p>
<p>Das Auslaufen und die Invalidierung von Accounts regelt jede Universität
selbst. <a class="footnote-reference brackets" href="#id38" id="id13">13</a> Der Nutzer hat danach keinen Zugriff mehr auf die
Ressourcen. Die Daten der Nutzer*innen verbleiben jedoch so lange auf
dem Cluster, bis die Ressource abgeschaltet wird oder die eine Anfrage
einer berechtigten Person erfolgt. Es gibt derzeit keine festen Regeln
diesbezüglich, so dass diese Frage einer genaueren Ausarbeitung Bedarf.</p>
</div>
<div class="section" id="kontingentierung">
<h2>Kontingentierung<a class="headerlink" href="#kontingentierung" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die Wissenschaftler*innen sind im Sinne der gemeinschaftlichen
DFG-Beantragung Stakeholder des bwForClusters NEMO. Zusätzlich gibt es
Shareholder die mit eigenen Mitteln Teile des Clusters mitfininaziert
haben ZKI-Aufwuchs. Diesen stehen
zusätzliche Anteile am Cluster zur Verfügung. Die Regelung, wer wie
viele Ressourcen des Clusters nutzen kann, wird über einen
„Fairshare-Mechanismus“ geregelt. <a class="footnote-reference brackets" href="#id39" id="id14">14</a> Dieser bestimmt wann ein Job
eines/r Wissenschaftler*in starten kann. Hierzu wird von einer Gruppe
jeweils der Verbrauch der letzten drei Monate mit ihrem „Share“
verglichen. Ist der Verbrauch höher als der Share, der der Arbeitsgruppe
zur Verfügung steht, werden die Jobs niedriger priorisiert, ist er
niedriger als der verfügbare Share, werden die Jobs höher priorisiert.
Wissenschaftler*innen können aber mehr Ressourcen verwenden, als ihnen
aufgrund ihres Shares zustehen würden. Sie werden dadurch in Zukunft nur
schlechter in der Warteschlange priorisiert. Es gibt lediglich eine
maximale Anzahl an Ressourcen, die ein/e Wissenschaftler*in in die
Warteschlange stellen können.</p>
</div>
<div class="section" id="administration">
<h2>Administration<a class="headerlink" href="#administration" title="Link zu dieser Überschrift">¶</a></h2>
<p>Administrator*innen verfügen über erweiterte Rechte. Sie haben Zugriff
auf alle Daten der Nutzer*innen, sofern diese nicht zusätzlich
verschlüsselt werden. Der Zugang wird bei Bedarf manuell gewährt und
wird bei Ausscheiden beziehungsweise wenn die Rechte nicht mehr benötigt
werden manuell entzogen. Derzeit wird ein Protokoll für die
Administration entwickelt, das diesen Aspekt regelt.</p>
</div>
<div class="section" id="monitoring-1">
<span id="id15"></span><h2>Monitoring<a class="headerlink" href="#monitoring-1" title="Link zu dieser Überschrift">¶</a></h2>
<p>Monitoring überwacht den dauerhaften Betrieb mit Verfolgung der Ziele
Verfügbarkeit, Vertraulichkeit und Integrität der Daten. Beim Monitoring
werden Schränke, Infrastrukturkomponenten wie Netzwerk, Speichersysteme,
Server und Rechenknoten überwacht. Neben der Überwachung der Hardware
wird die Temperatur, Stromaufnahme und zusätzlich bei Schränken die
Luftfeuchtigkeit kontrolliert. Die Nachverfolgung des Netzwerks findet
in der Netzwerkabteilung und bei Schränken in der Abteilung Allgemeiner
Betrieb statt. Strom und Kühlung werden zudem vom „Technischen
Gebäugemanagement“ (TGM) überwacht. Zusätzlich misst der
Monitoring-Server des Clusters mit Hilfe von Zabbix Hardwaredaten wie
Temperatur und Defekte auf Knotenebene und schlägt per Mail Alarm beim
Überschreiben von Grenzwerten. Zabbix überprüft zudem, ob die Dienste
noch auf den Servern noch zur Verfügung stehen. Hier wird nur geprüft,
ob der Dienst noch existiert.</p>
<p>Zusätzlich werden Hardware- sowie Softwareprobleme, Login- und
Zugriffsversuche lokal auf den Knoten und für die von den
Wissenschaftler*innen erreichbaren Knoten wie Login-, Vis- und
Rechenknoten zusätzlich auf dem Monitoringserver in Dateien gespeichert
und in konsolidierter Form auf einen gesicherten Bereich der
Storage-Appliance geschrieben.</p>
<p>Der Speicherverbrauch im parallelen Dateisystem und den
Home-Verzeichnissen wird mittels Quotas durchgesetzt. Die Auslastung
wird jeweils von den zuständigen Betreibern ermittelt. Bei Isilon ist
das die Abteilung „Virtualisierung und Speichersysteme“, beim BeeGFS
machen das die Administrator*innen des bwForClusters NEMO. „Workspaces“
auf dem parallelen Wissenschaftsspeicher BeeGFS haben einen Laufzeit von
und müssen von den Wissenschaftler*innen mit einem Kommando manuell
verlängert werden. Erfolgt das nicht, werden die Daten endgültig nach
einer Wartezeit von sieben Tagen gelöscht.</p>
</div>
</div>
<div class="section" id="verantwortlichkeiten">
<h1>Verantwortlichkeiten<a class="headerlink" href="#verantwortlichkeiten" title="Link zu dieser Überschrift">¶</a></h1>
<p>Die Verantwortung für den Betrieb des bwForClusters NEMO liegt bei
dem/der Leiter*in der Abteilung eScience. Diese/r berichtet der/dem
Leiter*in des Rechenzentrums der Universität Freiburg.</p>
<div class="section" id="maschinensaal-ii-msii">
<h2>Maschinensaal II (MSII)<a class="headerlink" href="#maschinensaal-ii-msii" title="Link zu dieser Überschrift">¶</a></h2>
<p>Der MSII sowie die darüber bereitgestellten Schränke werden innerhalb von
der Abteilung „Allgemeiner Betrieb“ verantwortet. Das operative Geschäft
sowie die organisatorischen Schnittstellen innerhalb des RZ sowie zu
Nutzer*innen, die Ressourcen im Maschinensaal betreiben, werden in der
„Maschinensaalbenutzungsordnung“ MSBO:2020 für
den Maschinensaal beschrieben. Die Nutzung der Server-Schränke wird im
Dienstkatalog „Machine-Hosting“ MachHost:2020
spezifiziert. Die Maschinensaalbenutzungsordnung bestimmt ebenfalls den
physikalischen Zugriff der Administrator*innen des Clusters auf die
Schränke und die darin eingebauten Maschinen.</p>
</div>
</div>
<div class="section" id="erwartete-ergebnisse">
<h1>Erwartete Ergebnisse<a class="headerlink" href="#erwartete-ergebnisse" title="Link zu dieser Überschrift">¶</a></h1>
<p>Mit dem Aufbau der Teildienste in technischen Schichten und der
flexiblen Boot-Prozedur können die strategischen Ziele des bwForClusters
NEMO mit den gegebenen Ressourcen erreicht und die
Informationssicherheit gewahrt werden. Die Gliederung der Schichten
erlaubt es, die Arbeitsbereiche zu trennen und die Risiken im Betrieb
einzelner Schichten besser zu isolieren. Besonders die im
Wissenschaftsbereich hohe Erwartung an Verfügbarkeit lässt sich besser
erreichen. Die Zahl der „Single Points of Failures“ ist besser
kontrollierbar. Die Standardisierung in der Steuerung der Hardware
reduziert die Komplexität im Betrieb, die den Wissenschaftler*innen
gebotene Freiheit ist praktisch vollständig von der Betriebsschicht
getrennt. Die Abstraktion reduziert Angriffsvektoren auf die
Betriebsschicht, die durch Ereignisse auf der Ebene der
Wissenschaftler*innen eröffnet werden.</p>
</div>
<div class="section" id="ziele-im-sinne-der-informationssicherheit">
<h1>Ziele im Sinne der Informationssicherheit<a class="headerlink" href="#ziele-im-sinne-der-informationssicherheit" title="Link zu dieser Überschrift">¶</a></h1>
<p>Kernanliegen von bwForcluster NEMO ist, Forscher*innen den Zugang zu
Rechen- und Speicherkapazitäten zu geben. Als Schutzziel betrachtet ist
dies die Sicherstellung der <em>Verfügbarkeit</em> von Rechenressourcen, auf
denen Forscher*innen ihre Rechenjobs ausführen. Als Dienstanbieter muss
das bwForCluster NEMO eine Verfügbarkeit bieten, die Forscher*innen
Big-Data-Analysen über das Maß selbst gemanagter oder dezentral in
Forschungseinrichtung administrierten IT-Cluster hinaus ermöglicht.</p>
<p>Für die Forscher*innen ist, um ihre Datenverarbeitung und Ergebnisse
reproduzierbar zu halten, eine <em>Integrität</em> von Daten auf Speicherebene
notwendig. Für Integrität von Daten auf der von ihnen betriebenen
Verarbeitungsschicht sind sie selbst verantwortlich.</p>
<p>Das Schutzziel <em>Vertraulichkeit</em> bezieht sich auf die Kontrolle des
Zutritts, Zugangs und Zugriffs und die Trennung von Umgebungen. Mit der
Abstraktion der Dienstschichten – Konzentration auf der Ebene der
physischen und netztechnischen Infrastrukturen, der
Clusteradministration in HPC, der Entkoppelung von logischen
Nutzer-bezogenen Einheiten – sind Arbeitsteilungen möglich, die ein
größeres Spektrum an Diensten für die Forschung in den Bereich des
wirtschaftlich möglichen schieben. Sicherheitsrelevante Arbeitsbereiche
können zentral von qualifiziertem Personal abgedeckt werden.</p>
<p>Regulatorische Anforderungen an Forschung im Bereich der
Neurowissenschaft, die in einigen Bereichen personenbezogene
medizinische Daten mit hohem Schutzbedarf erzeugen, erfordern die
Sicherstellung, diese Daten innerhalb eines kontrollierbaren Rechtsraums
zu speichern. Die Souveränität wird auf die Kontrolle der physischen,
organisatorischen und operativen Aspekte bezogen.</p>
<p>Das bwForCluster NEMO ist mit diesen Überlegungen in der Lage, die aus
den strategischen Zielen abgeleiteten Schutzziele mit eigenen Maßnahmen
wirtschaftlich tragen und erreichen zu können.</p>
<div class="section" id="vertraulichkeit">
<h2>Vertraulichkeit<a class="headerlink" href="#vertraulichkeit" title="Link zu dieser Überschrift">¶</a></h2>
<p>ist über die Maschinensaalbenutzungsordnung (MsBO) festgelegt. <a class="footnote-reference brackets" href="#id40" id="id16">15</a>
Diese regelt folgende Aspekte:</p>
<ul class="simple">
<li><p>Zugang ist nur für berechtigte Personen mittels Transponder möglich.</p></li>
<li><p>Die Vergabe der Schlüssel und Transponder ist dokumentiert.</p></li>
<li><p>MSII ist Alarmgesichert gegen Zutritt unbefugter Personen.</p></li>
<li><p>MSII verfügt über Rauchmelder.</p></li>
<li><p>Während der Öffnungszeiten ist Zutritt in MSII nur über eine
Magnetkarte oder einen Schlüssel möglich.</p></li>
<li><p>Außerhalb der Öffnungszeiten wird das Rechenzentrum regelmäßig von
einem Sicherheitsdienst kontrolliert.</p></li>
<li><p>Zutritt für Wartungen durch externe Parteien wird dokumentiert.</p></li>
</ul>
<p>zum bwForCluster NEMO erfolgt über mehrere Stufen. Das genauen Verfahren
ist im bwHPC-Wiki beschrieben. <a class="footnote-reference brackets" href="#id41" id="id17">16</a> Die genauen Maßnahmen beinhalten:</p>
<ul class="simple">
<li><p>Wissenschaftler*innen müssen von der Universität. berechtigt sein die
Ressource zu nutzen. Hierzu wird von den Universitäten das
Entitlement „bwForCluster“ vergeben. <a class="footnote-reference brackets" href="#id42" id="id18">17</a></p></li>
<li><p>Wissenschaftler*innen müssen einen Projektantrag stellen
(Rechenvorhaben).</p></li>
<li><p>Wissenschaftler*innen müssen sich beim Cluster registrieren, dazu
werden Daten beim jeweiligen Identity-Provider (IdP) abgefragt. <a class="footnote-reference brackets" href="#id43" id="id19">18</a>
Diese Daten können auch jederzeit von den Nutzer*innen abgefragt
werden. <a class="footnote-reference brackets" href="#id44" id="id20">19</a> Hierzu gehören:</p>
<ul>
<li><p>Name</p></li>
<li><p>E-Mail-Adresse</p></li>
<li><p>EPPN <a class="footnote-reference brackets" href="#id45" id="id21">20</a></p></li>
<li><p>Berechtigungen über „Entitlements“.</p></li>
<li><p>Eine Unix-Gruppe, falls diese vom IdP ausgeliefert wird.</p></li>
</ul>
</li>
<li><p>Der Zugang ist über eine Firewall auf Belwü-Netze beschränkt. <a class="footnote-reference brackets" href="#id46" id="id22">21</a></p>
<ul>
<li><p>Außerhalb dieser Netze ist der Zugriff nur über Jump-Hosts
(beispielsweise Proxy-Jump) oder VPN möglich.</p></li>
</ul>
</li>
<li><p>Wissenschaftler*innen haben Zugriff von außen über SSH mit Nutzername
und Passwort beziehungsweise SSH-Schlüssel auf Login- und
Visualisierungsknoten.</p></li>
<li><p>Zugang über das Openstack-Dashboard über HTTPS mit Nutzername und
Passwort.</p></li>
<li><p>Administrator*innen haben nur mit Nutzername und SSH-Schlüssel
Zugriff.</p></li>
<li><p>Eine Zwei-Faktor-Authentifizierung wird derzeit ausgerollt.</p></li>
<li><p>Der Zugang wird über Log-Dateien lokal und im Monitoring-Server
protokolliert.</p></li>
</ul>
<p>erfolgt auf Ebene von Unix-Rechten und „Access Control Lists“ (ACL).
Folgende Regeln gelten:</p>
<ul class="simple">
<li><p>Wissenschaftler*innen haben nur auf ihre eigenen Daten Zugriff. Diese
Berechtigungen können selbst für weitere Nutzer*innen, beispielsweise
über ACLs, erweitert werden.</p></li>
<li><p>Administratoren haben auf alle Daten und Bereiche Zugriff, wenn sie
nicht von Client- oder Nutzerseite verschlüsselt werden.</p></li>
<li><p>Externe Supportmitarbeiter haben nur Zugriff auf die von ihnen zu
wartenden Komponenten.</p></li>
<li><p>Testsysteme und weitere Untersuchungen werden nur ausgewählten
Personengruppen zur Verfügung gestellt und beeinträchtigen nicht den
Produktivbetrieb.</p></li>
<li><p>Zentrales Benutzer-/Berechtigungs-Management folgt dem
Need-to-have-Prinzip für Wissenschaftler*innen, externen Support,
Projektmitarbeiter*innen im bwHPC, Testnutzer*innen und
Administrator*innen.</p></li>
<li><p>Die Wissenschafler*innen verlieren ihren Zugriff, sobald dieser von
der Universität entzogen wird beziehungsweise der Account nicht mehr
gültig ist. Die Regeln, wann dies erfolgt, werden von den
Universitäten festgelegt.</p></li>
<li><p>Weitere zusätzliche Rechte beispielsweise Administratorrechte werden
manuell entzogen. Ein Protokoll hierzu wird derzeit ausarbeitet.</p></li>
</ul>
<p>gewährleistet, dass zu unterschiedlichen Zwecken erhobene Daten getrennt
verarbeitet werden können. Hierzu zählen folgende Maßnahmen:</p>
<ul class="simple">
<li><p>Physikalische und logische Trennung von Diensten, die nicht
unmittelbar miteinander in Bezug stehen.</p></li>
<li><p>Physikalische und logische Trennung von Diensten und Netzen, die
nicht aufeinander zugreifen müssen.</p></li>
</ul>
</div>
<div class="section" id="integritat">
<h2>Integrität<a class="headerlink" href="#integritat" title="Link zu dieser Überschrift">¶</a></h2>
<p>werden Maßnahmen bezeichnet, die ein unbefugtes Lesen, Kopieren,
Verändern oder Entfernen bei elektronischer Übertragung oder Transport
verhindern. Das bwForCluster NEMO ist nur über folgende Protokolle und
Wege erreichbar:</p>
<ul class="simple">
<li><p>Zugriff auf das Cluster ist auf das Belwü-Netz beschränkt.</p></li>
<li><p>Außerhalb des Belwü-Netzes muss VPN oder ein Jump-Host im Belwü
verwendet werden.</p></li>
<li><p>Zugriffe werden auf Serverseite protokolliert.</p></li>
<li><p>Zugriff kann nur über verschlüsselte Dienste wie SSH und HTTPS
erfolgen.</p></li>
<li><p>Die lokale Festplatte der Rechenknoten wird beim Booten verschlüsselt
und kann nach Entfernen nicht ausgelesen werden.</p></li>
<li><p>Die Platten im Parallelspeicher enthalten nur teilweise Blöcke und
eine Rekonstruktion ist nur möglich, wenn größere Teile entwendet
werden. Jedoch ist für das Nachfolgesystem evtl. eine Verschlüsselung
geplant.</p></li>
<li><p>Alle lokalen Datenträger auf per Remote-Boote gesteuerten Maschinen
werden mit einem zufällig erzeugten individuellen Schlüssel lokal
verschlüsselt aufgesetzt. Damit liegen keine auslesbaren
Informationen auf einer ausgeschalteten Maschine vor.</p></li>
<li><p>Updates des Basissystems und Softwarekomponenten</p></li>
<li><p>Die sichere Entsorgung von Datenträgern ist in der MsBO
geregelt. <a class="footnote-reference brackets" href="#id47" id="id23">22</a></p></li>
</ul>
</div>
<div class="section" id="verfugbarkeit-belastbarkeit">
<h2>Verfügbarkeit, Belastbarkeit<a class="headerlink" href="#verfugbarkeit-belastbarkeit" title="Link zu dieser Überschrift">¶</a></h2>
<p>zählen Maßnahmen, die eine zufällige Zerstörung oder Verlust von Daten
und die Nutzbarkeit der Rechenressourcen beschreiben. Das bwForCluster
NEMO implementiert folgende Schutzmaßnahmen:</p>
<ul class="simple">
<li><p>Feuer- und Rauchmeldeanlagen in MSII und Infrastrukturräumen wie
Kühlung und Strom.</p></li>
<li><p>Redundante Kühlung bis zur Abschaltung für Cluster-kritische Dienste
wie Parallelspeicher, HOME-Speicher und Server für Dienste.
Rechenknoten sind nicht geschützt, können aber nach Behebung der
Störung sofort wieder hochgefahren werden. Da im Desasterfall der
Parallelspeicher und die Dienste vermutlich nicht benötigt werden,
können diese unter Umständen vorsichtshalber sicher herunter gefahren
werden.</p></li>
<li><p>Für Server und Speicher des bwForClusters NEMO besteht
unterbrechungsfreie Stromversorgung sowie Notstromversorgung.</p></li>
<li><p>Die Temperatur, Feuchtigkeit und Stromverbrauch der Maschinen und der
Datenschränke werden überwacht.</p></li>
<li><p>Das HOME-Verzeichnis der Wissenschaftler*innen ist georedundant
gespeichert und bietet automatische Snapshots. <a class="footnote-reference brackets" href="#id48" id="id24">23</a></p></li>
<li><p>Der Parallelspeicher ist mit RAID-6 abgesichert.</p></li>
<li><p>Der Zugriff auf die Login- und Vis-Knoten ist durch eine Firewall und
Fail2ban gesichert. Der Zugriff auf das Cloud-Dashboard über eine
Firewall abgesichert. Die Restlichen Komponenten sind nur
Cluster-intern erreichbar. Ausfälle durch Angriffe externer Parteien
können so minimiert werden.</p></li>
<li><p>Der Zugriff auf das bwForCluster NEMO ist nur Wissenschaftler*innen
aus baden-württembergischen Universitäten, wenigen
Administrator*innen und Support-Mitarbeitern erlaubt, was den
Angriffsvektor zusätzlich verkleinert.</p></li>
<li><p>Die Verfügbarkeit des bwForClusters NEMO wird überwacht.</p></li>
<li><p>Das Wiederanfahren des Systems kann nach „Ausfällen ohne
Datenverlust“ innerhalb weniger Stunden erfolgen.</p></li>
</ul>
</div>
<div class="section" id="regelmaszige-uberprufung-bewertung-evaluation">
<h2>Regelmäßige Überprüfung, Bewertung, Evaluation<a class="headerlink" href="#regelmaszige-uberprufung-bewertung-evaluation" title="Link zu dieser Überschrift">¶</a></h2>
<p>Die Universität Freiburg nimmt den Schutz der ihr anvertrauten
personenbezogenen Daten sehr ernst und behandelt diese vertraulich und
entsprechend der gesetzlichen Vorschriften. Neben den Regelungen der
Europäischen Datenschutz-Grundverordnung (DSGVO) richtet sich die
Verarbeitung personenbezogener Daten an der Universität nach dem
Landesdatenschutzgesetz (LDSG) sowie den einschlägigen Regelungen des
Landeshochschulgesetzes (LHG).</p>
<p>Die Datenschutzbeauftragte Person der Universität Freiburg kann unter
der E-Mail-Adresse</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>datenschutzbeauftragter@uni-freiburg.de
</pre></div>
</div>
<p>sowie unter der Postadresse der Universität mit dem Zusatz „Der
Datenschutzbeauftragte“ erreicht werden. Allgemeinen Fragen zum Thema
Datenschutz können an die E-Mail-Adresse</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>datenschutz@uni-freiburg.de
</pre></div>
</div>
<p>gerichtet werden.</p>
<p>Dazu arbeitet die Universität weiterhin mit der Zentralen
Datenschutzstelle der baden-württembergischen Universitäten (ZENDAS)
zusammen.</p>
<p>unterstützt bei der Reaktion auf Sicherheitsverletzungen. Hierzu zählen
beim bwForCluster NEMO:</p>
<ul class="simple">
<li><p>Meldung von Sicherheitsvorfällen beim Sicherheitsbeauftragten und
Datenschutzbeauftragten der Universität, bei den Projektpatrnern im
bwHPC und dem DFNCert.</p></li>
<li><p>Das DFNCert untersucht Angriffe durch externe Parteien.</p></li>
</ul>
<p>Für die Registrierung beim bwForCluster NEMO werden nur so viele
personenbezogene Daten erhoben, wie für den Dienst notwendig sind, siehe
Abschnitt Zugangskontrolle. <a class="footnote-reference brackets" href="#id49" id="id25">24</a></p>
</div>
</div>
<div class="section" id="bezuge-zu-konzepten-richtlinien-und-verordnungen">
<h1>Bezüge zu Konzepten, Richtlinien und Verordnungen<a class="headerlink" href="#bezuge-zu-konzepten-richtlinien-und-verordnungen" title="Link zu dieser Überschrift">¶</a></h1>
<dl class="footnote brackets">
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Vgl. „Compute-Forschungsinfrastrukturen: HPC“,
<a class="reference external" href="https://bwsyncandshare.kit.edu/s/t3gWxTnnjYKD9fS">https://bwsyncandshare.kit.edu/s/t3gWxTnnjYKD9fS</a></p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Siehe HPC-Dienstbeschreibung, Version vom 07.07.2016,
<a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschreibung-hpc">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/dienstbeschreibung-hpc</a>
(Überarbeitungsversion:
<a class="reference external" href="https://bwsyncandshare.kit.edu/s/RMnZSCbJaJXgQGw">https://bwsyncandshare.kit.edu/s/RMnZSCbJaJXgQGw</a>)</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Beim Distributed Network Block Device 3 (DNBD3) handelt es sich um
ein spezielles, auf das Lesen optimiertes, verteiltes, blockbasiertes
Speichersystem bwhpc2018:dnbd3.</p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Rechenzentrum Universität Freiburg, Hermann-Herder-Str. 10,
79104 Freiburg</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>TODO</p>
</dd>
<dt class="label" id="id31"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p>Vgl. Dienstbeschreibung vom 28.01.2019,
<a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/speichersysteme">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/speichersysteme</a></p>
</dd>
<dt class="label" id="id32"><span class="brackets"><a class="fn-backref" href="#id7">7</a></span></dt>
<dd><p><a class="reference external" href="beegfs.io">beegfs.io</a></p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id8">8</a></span></dt>
<dd></dd>
<dt class="label" id="id34"><span class="brackets"><a class="fn-backref" href="#id9">9</a></span></dt>
<dd></dd>
<dt class="label" id="id35"><span class="brackets"><a class="fn-backref" href="#id10">10</a></span></dt>
<dd><p>Wiki-Eintrag zu LACP: <a class="reference external" href="https://de.wikipedia.org/wiki/Link_Aggregation">https://de.wikipedia.org/wiki/Link_Aggregation</a>,
besucht am 19.02.2021.</p>
</dd>
<dt class="label" id="id36"><span class="brackets"><a class="fn-backref" href="#id11">11</a></span></dt>
<dd><p>Eintrag zu Omni-Path: <a class="reference external" href="https://de.wikipedia.org/wiki/Intel_Omni-Path">https://de.wikipedia.org/wiki/Intel_Omni-Path</a>,
besucht am 19.02.2021.</p>
</dd>
<dt class="label" id="id37"><span class="brackets"><a class="fn-backref" href="#id12">12</a></span></dt>
<dd><p>WIKI</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id13">13</a></span></dt>
<dd><p>TODO: Links zu NO</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id14">14</a></span></dt>
<dd><p>FAIRSHARE</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id16">15</a></span></dt>
<dd><p>Vgl. <a class="reference external" href="https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/msbo">https://www.rz.uni-freiburg.de/inhalt/dokumente/pdfs/msbo</a>
Version vom 24.09.2020</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id17">16</a></span></dt>
<dd><p>TODO Wiki</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id18">17</a></span></dt>
<dd><p>bwidm seite ent</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id19">18</a></span></dt>
<dd><p>link zu bwservices bwz bwidm</p>
</dd>
<dt class="label" id="id44"><span class="brackets"><a class="fn-backref" href="#id20">19</a></span></dt>
<dd><p><a class="reference external" href="https://bwservices.uni-freiburg.de/user/index.xhtml">https://bwservices.uni-freiburg.de/user/index.xhtml</a></p>
</dd>
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id21">20</a></span></dt>
<dd><p>bwidm</p>
</dd>
<dt class="label" id="id46"><span class="brackets"><a class="fn-backref" href="#id22">21</a></span></dt>
<dd><p>die genaue nummer und www verlinken</p>
</dd>
<dt class="label" id="id47"><span class="brackets"><a class="fn-backref" href="#id23">22</a></span></dt>
<dd><p>TODO Nicht in der MsBO, wo?</p>
</dd>
<dt class="label" id="id48"><span class="brackets"><a class="fn-backref" href="#id24">23</a></span></dt>
<dd><p>INFO zu SNAPSHOTS</p>
</dd>
<dt class="label" id="id49"><span class="brackets"><a class="fn-backref" href="#id25">24</a></span></dt>
<dd><p>ZAS, bwIDM</p>
</dd>
</dl>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="LICENSE.html" class="btn btn-neutral float-right" title="MIT License" accesskey="n" rel="next">Weiter <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index.html" class="btn btn-neutral float-left" title="Betriebskonzept bwForCluster NEMO" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Zurück</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, eScience, Rechenzentrum, Albert-Ludwigs-Universität Freiburg.

    </p>
  </div>
    
    
    
    Erstellt mit <a href="https://www.sphinx-doc.org/">Sphinx</a> mit einem
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    bereitgestellt von <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>